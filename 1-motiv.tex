\section{Motivation and Challenges}
\label{sec:motiv}

In this section, we examine the issues with synchronous communications in choreographic programming languages and give a high-level overview of our extension and its challenge.
%
We use Async HaChor for examples but omit some uninteresting details.

\subsection{Issues with Synchronous Communications}

Existing choreographic programming languages often assume a model where communications are synchronous~\citep{CC, chor-lambda, pirouette}.
%
The synchronous behavior is evident from the type signature of the communication operator:
%
\begin{minted}{haskell}
  comm :: forall (s :: Loc) (r :: Loc). a @ s -> Choreo (a @ r)
\end{minted}
%
\texttt{comm} takes a sender and receiver location and moves a value from the sender (\hs{a @ s}) to the receiver (\hs{a @ r}) in the \hs{Choreo} monad.
%
The value is immediately usable at the receiver, indicating the communication has completed.
%
While easy to reason about and simple to implement, synchronous communication significantly restricts the efficiency and expressivity of a choreography:
%
\begin{itemize}
  \item
  \textbf{Issue 1.} The synchronous assumption limits the amount of concurrency a choreography can have since both the sender and receiver are blocked until the communication is completed.
  %
  For example, the data splitting phase of a MapReduce-like~\citep{map-reduce} system can be represented as the following choreography:
  %
  \begin{minted}{haskell}
  split = \bigData -> do
    (c1, c2, c3) <- locally @Leader (partition bigData 3)
    comm @Leader @Worker1 c1
    comm @Leader @Worker2 c2
    comm @Leader @Worker3 c3
  \end{minted}
  %
  The \texttt{split} choreography takes in some data at the leader, partitions it into chunks, and distributes them across three workers.  
  %
  The code clearly expresses the functionality of the application.
  %
  However, the performance is suboptimal as all the communications are executed sequentially, making the total completion time the \emph{sum} of all network requests.
  %
  This becomes particularly problematic when dealing with data in big size.
  %
  Ideally, we want to exploit concurrency and overlap these communications.

  \item
  \textbf{Issue 2.} The synchronous assumption makes it impossible to handle message losses or machine crashes, which are very common in distributed systems and crucial for achieving fault tolerance.
  %
  For example, consider a simplified president election protocol where a candidate needs a \emph{quorum} of yes votes (usually a majority) from voters A, B, and C to get elected.
  %
  It might be tempting to define the protocol as the following choroegraphy:
  %
  \begin{minted}{haskell}
  election = do
    a <- comm @VoterA @Candidate vote
    b <- comm @VoterB @Candidate vote
    c <- comm @VoterC @Candidate vote
    passed <- locally @Candidate (checkMajority a b c)
    cond passed $ \case
      True  -> locally @Candidate (putStrLn "I win :-)")
      False -> election
  \end{minted}    
  %
  The \texttt{election} choreography starts with each voter sending their newly generated vote to the candidate~\footnote{The actual code needs to use \texttt{comm'} here since \texttt{vote} is stateful.}, and then the candidate checks locally if they have received a majority of yes votes.
  %
  If true, the candidate delcares their victory; otherwise, the election is held again.
  %
  The election should be able conclude even if a voter is unavailable.
  %
  However, the execution of \texttt{checkMajority} requires all votes to be received as it is blocked by the three proceeding \texttt{comm}.
  %
  So, if voter A is down or the network to it is significantly delayed, the candidate can not be elected even if the other two vote yes.
\end{itemize}

Some choreographic programming models relax the synchronous assumption by creating a new thread for each message send, as seen in the asynchronous extension in \citet{montesi-textbook}.
%
This addresses the first issue but not the second --- receiving is still blocking.
%
We propose making both sending and receiving non-blocking.

\subsection{Bring Choreographic Programming into the Future}

To make both sending and receiving non-blocking, we create a new thread for both each message send and receive and use a \emph{future} to represent the yet-to-arrive message.
%
On the interface level, this amounts to changing the type signature of \texttt{comm} to return an \emph{Async} (Haskell's equivalent of a future):
%
\begin{minted}{haskell}
  comm :: forall (s :: Loc) (r :: Loc). a @ s -> Choreo (Async a @ r)
\end{minted}
%
Now a call of \texttt{comm} will immediately return and perform the communication asynchronously in the background.
%
The \texttt{Async} type comes with a group of wait functions, such as \texttt{wait}, \texttt{waitBoth}, and \texttt{waitAny}, to retrive the result of it under different conditions.
%
Thanks to the fact that Async HasChor is a library-level language, we do not need to write our own asynchronus runtime and can piggyback on the off-the-shelf \texttt{async} package from Haskell.

With the help of \texttt{Async}, we can now implement the \texttt{checkMajor} funciton in the \texttt{election} protocol as:
%
\begin{minted}{haskell}
  checkMajority x y z = do
    (async1, vote1) <- waitAnyExcept [] [x, y, z]  
    (async2, vote2) <- waitAnyExcept [async1] [x, y, z]
    if vote1 && vote2              
    then return True
    else if not vote1 && not vote2
    then return False
    else do                                  
      (vote3, async3) <- waitAnyExcept [async1, async2] [x, y, z]
      return vote3
\end{minted}

\subsection{Get the Right Message}

The introduction of futures allow communications to happen asynchronously, which implies they may complete in a non-deterministic order at run time.
%
This poses a significant challenge to endpoint projection: how can we determine which variable/future a received message corresponds to?
%
An naive attempt that 